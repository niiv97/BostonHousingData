# -*- coding: utf-8 -*-
"""Busa 715 Final Project - Boston Housing Study (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sfk0f6PYdNK1Vm3oEmRKS0np_f-NQZFP

## BOSTON HOUSING CLUSTERING PROJECT

### Objective
#### This project's objective is to assist individuals who are interested in buying or renting a home in Boston and those who want to make investment decisions in the housing market. To achieve this objective, we analyze the market dynamics in different towns and identify critical factors such as the number of rooms per dwelling, age of buildings, median home value, crime rate in the neighborhood, and accessibility to radial highways.
"""

# Import all the necessary packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Import machine learning and clustering algorithms
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

from sklearn.cluster import AgglomerativeClustering
# to perform hierarchical clustering, compute cophenetic correlation, and create dendrograms
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage, cophenet
# to compute distances
from scipy.spatial.distance import pdist

# Import utilities for computing distances
from scipy.spatial.distance import cdist

# To suppress warnings
import warnings
warnings.filterwarnings("ignore")

df_1 = pd.read_csv("boston.csv")

df = df_1.copy()
df.shape

"""#### Boston dataset contains 506 rows and 13 columns"""

df.head(5)

"""#### Looking at the first 5 rows of our dataset"""

df.info()

"""#### Boston dataset contains 13 variables. All of them are numerical.
#### Variables names

•	CRIM: per capita crime rate by town

•	ZN: proportion of residential land zoned for lots over 25,000 sq. ft.

•	INDUS: proportion of non-retail business acres per town

•	CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)

•	NOX: nitric oxides concentration (parts per 10 million)

•	RM: average number of rooms per dwelling

•	AGE: proportion of owner-occupied units built prior to 1940

•	DIS: weighted distances to five Boston employment centers

•	RAD: index of accessibility to radial highways

•	TAX: full-value property-tax rate per 10,000 dollars

•	PTRATIO: pupil-teacher ratio by town

•	LSTAT: %lower status of the population

•	MEDV: Median value of owner-occupied homes in 1000 dollars.
"""

df.describe()

# dropping/ignoring object variable columns
df = df.drop(columns=["CHAS"])

"""#### Dropping the variable CHAS ( Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) because it doesn't add any value to the clusterong process."""

# checking for missing values
df.isnull().sum()

"""#### There are no missing values in the Boston Houses dataset."""

df.duplicated().sum()

"""#### There are no duplicates in our dateset.

# EDA
"""

df_numeric = df.select_dtypes(include=['number'])

# Loop through numeric columns
for col in df_numeric.columns:
    print(col)
    print("Skew :", round(df_numeric[col].skew(), 2))
    plt.figure(figsize=(15, 4))
    plt.subplot(1, 2, 1)
    df_numeric[col].hist(bins=10, grid=False)
    plt.ylabel("count")
    plt.subplot(1, 2, 2)
    sns.boxplot(x=df_numeric[col])
    plt.show()

"""### EDA Analysis

#### The positively skewed distribution of the CRIM and ZN variables suggests that the majority of the areas in the dataset have relatively lower crime rates and residential plots of smaller sizes, with only a few areas exhibiting higher crime rates and larger sizes. This distribution could indicate potential patterns in urban development or demographic characteristics of the areas represented in the dataset.
#### On average, the proportion of non-retail business acres per town stands at 11.37, with a minimum of 0.46 and a maximum of 27.74. These figures suggest that most of the town are predominantly residential or suited for retail businesses. It implies a limited presence of industrial activity in this area, with a greater emphasis on residential living and small-scale commercial establishments.
#### The average level of nitric oxide conentration for these towns is 0.554 which indicates fluctuation between clear air and higher lavels of pollution. It's importat to take in consideration this factor to ensure the well being of the residents.
#### The average of 6 rooms per dwelling suggests that many of them are likely spacious and suitable for families or larger households. The minimum of 3 rooms may represent smaller or more modest housing options, while the maximum of 8 rooms may indicate larger or more luxurious dwellings. This variety in the number of rooms per dwelling reflects different housing preferences and socioeconomic factors within the communit.
#### The distribution of the Age variable (proportion of owner-occupied units built before 1940) suggest that most of these houses were build before 1940.
#### The DIS variable, which represents the weighted distances to five Boston employment centers, suggests that the majority of these houses are situated in close proximity to employment hubs. This is advantageous as it implies shorter commute times for families residing in these areas.
#### With an average full-value property tax rate of 408 per 10,000 and a standard deviation of 168.54, it's evident that property tax rates exhibit significant variability within the dataset. While the mean tax rate stands at 408, individual properties can experience substantial deviations from this average, with some facing notably higher tax burdens.
#### The RAD (index of accessibility to radial highways) ranges from 1 to 24, with a mean value of 9.55, suggesting a medium to high level of accessibility to radial highways for the majority of the town's houses.
#### The average pupil-teacher ratio by town (TRATIO) is 18.46, which is favorable as it falls below the desired level of less than 30. In summary, it can be concluded that education standards are generally satisfactory in most of these towns.This particular variable holds significance for families with children when determining their relocation choices.
#### The plot for LSTAT: %lower status of the population shows a notable variability of the status of the residents from these towns. However, the wide range suggests that there are areas with significantly lower socioeconomic status, potentially impacting various socio-economic factors within these towns.
#### The distribution of MEDV appears to be slightly skewed to the right, resembling a nearly normal distribution. The average value of owner-occupied homes, measured in thousands of dollars, is approximately 23. Additionally, the plot indicates some outliers, further confirming the observed right-skewed distribution.

### Heat Map
"""

# generating a heatmap
plt.figure(figsize=(15, 7))
sns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral")
plt.show()

"""#### As we can see on the above heat map, the following variables are positively correlated.
#### -RM & MEDV
#### -TAX & INDUS
#### -NX &INDUS
#### Below we can see the other variables that has high negative correlation.
#### -DIS &NX
#### -DIS & INDUS
#### -AGE & DIS
#### -LSTAT & MEDV

# Normalizing the data
"""

scaler=StandardScaler()
df_scaled=pd.DataFrame(scaler.fit_transform(df), columns=df.columns)

"""#### We aim to assess clusters ranging from 1 to 9. Our goal is to determine the ideal cluster count by plotting the mean distortion for each scenario."""

# step 1
WCSS = {}

# step 2 - iterate for a range of Ks and fit the scaled data to the algorithm. Use inertia attribute from the clustering object and
# store the inertia value for that k
for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(df_scaled)
    WCSS[k] = kmeans.inertia_

# step 3
plt.figure()
plt.plot(list(WCSS.keys()), list(WCSS.values()), 'bx-')
plt.xlabel("Number of cluster")
plt.ylabel("WCSS")
plt.show()

"""#### Visual inspection suggests elbow at 3 clusters. Let us use Knee Locator"""

# Uncomment the following code to install kneed
!pip install kneed

from kneed import KneeLocator #importing kneelocator from kneed library

k_values = list(WCSS.keys())
wcss_values = list(WCSS.values())
k1 = KneeLocator(k_values, wcss_values, curve="convex", direction="decreasing")

# Find the elbow point
elbow_point = k1.elbow
print("Elbow point at:", elbow_point)

"""# Using the Silhouette method"""

from sklearn.metrics import silhouette_score

silhouette_coefficients = []
for k in range (2, 11):
    kmeans = KMeans (n_clusters = k, random_state=42)
    kmeans.fit (df_scaled)
    score = silhouette_score (df_scaled, kmeans.labels_)
    silhouette_coefficients.append (score)
# using the silhouette method

# Plotting the results:
plt.plot (range (2, 11), silhouette_coefficients)
plt.xticks (range (2, 11))
plt.xlabel ("Number of Clusters")
plt.ylabel ("Silhouette Coefficient")
plt.grid(True)
plt.show ()

"""#### From the Silhouette score plot showed above we can see that the optimal number is cluster in this case is 3 clusters."""

#### Creatin

kmeans = KMeans(n_clusters=3, random_state=1)  # initializing the model
kmeans.fit(df_scaled)

#Adding labels to scaled and original data
df_scaled["kmeans_clusters"] = kmeans.labels_
df['kmeans_clusters'] = kmeans.labels_

df_scaled.head()#displaying the scaled features of the dataset

fig = plt.figure(figsize=(8,5), dpi=100)
sns.scatterplot(x='AGE', y='DIS', data=df_scaled, hue='kmeans_clusters', palette=['blue', 'green', 'red'])
plt.title('AGE vs DIS', fontsize=12)
plt.xticks(rotation=90, fontsize=10)
plt.yticks(fontsize=10)
plt.xlabel('AGE', fontsize=10)
plt.ylabel('DIS', fontsize=10)
plt.show()

cluster_profile = df_scaled.groupby("kmeans_clusters").mean()

cluster_profile["count_in_each_segment"] = (
    df_scaled.groupby("kmeans_clusters")["AGE"].count().values
)

#Number of observations in each cluster
df_scaled.kmeans_clusters.value_counts ()

# let's display cluster profiles
cluster_profile.style.highlight_max(color="yellow", axis=0)

# plotting means in scaled data
df_scaled.groupby("kmeans_clusters").mean().plot.bar(figsize=(12, 18))

"""# Hierarchical Clustering Method"""

df.head()



fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(10, 7), squeeze=False)
for i, method in enumerate(
    ["single", "complete", "average", "centroid", "weighted", "ward"]
):
    ax = axes[i // 2, i % 2]
    Z = linkage(df_scaled, method=method)
    dendrogram(Z, color_threshold=0, ax=ax, no_labels=True)
    ax.set_xlabel(method)

sns.clustermap(df_scaled, figsize=(7, 5))

sns.clustermap(
    df_scaled.T,
    metric="cityblock",
    method="average",
    row_cluster=False,
    col_cluster=True,
    cmap="YlGnBu",
    figsize=(7, 5),
)

# list of distance metrics
distance_metrics = ["euclidean", "chebyshev", "mahalanobis", "cityblock"]
# list of linkage methods
linkage_methods = ["single", "complete", "average", "weighted"]

high_cophenet_corr = 0
high_dm_lm = [0, 0]

for dm in distance_metrics:
    for lm in linkage_methods:
        Z = linkage(df_scaled, metric=dm, method=lm)
        c, coph_dists = cophenet(Z, pdist(df_scaled))
        print(
            "Cophenetic correlation for {} distance and {} linkage is {}.".format(
                dm.capitalize(), lm, c
            )
        )
        if high_cophenet_corr < c:
            high_cophenet_corr = c
            high_dm_lm[0] = dm
            high_dm_lm[1] = lm

# printing the combination of distance metric and linkage method with the highest cophenetic correlation
print("Highest cophenetic correlation is {}, which is obtained with {} distance and {} linkage.".format(high_cophenet_corr, high_dm_lm[0], high_dm_lm[1]))

# Hierarchical clustering using complete linkage and Euclidean distance
Z = linkage(df_scaled, method="average", metric="euclidean")
fig = plt.figure(figsize=(10, 6))
dendrogram(Z, no_labels=True, color_threshold=3)
plt.show()

# Let us visualze the dendrogram
plt.figure(figsize=(10, 6))
plt.title("Agglomerative Hierarchical Clustering Dendogram")
plt.xlabel("sample index")
plt.ylabel("Distance")
dendrogram(Z,leaf_rotation=90.0, leaf_font_size=5.0)
plt.tight_layout
plt.show

HCmodel = AgglomerativeClustering(n_clusters=4, affinity="euclidean", linkage="average")
HCmodel.fit(df_scaled)

# adding hierarchical cluster labels to the original dataframes
df["HC_Clusters"] = HCmodel.labels_

df.sample(5)

cluster_profile_HC = df.groupby("HC_Clusters").mean()

cluster_profile_HC["count_in_each_segments"] = (
    df.groupby("HC_Clusters")["AGE"].count().values
)

# let's display cluster profiles
cluster_profile_HC.style.highlight_max(color="pink", axis=0)

# plotting of mean of every variable in each cluster
df.groupby("HC_Clusters").mean().plot.bar(figsize=(12, 5))

"""### K-Means with PCA"""

pca=PCA()
pca.fit(df_scaled)
print(pca.components_)

print(pca.explained_variance_ratio_)

print(np.cumsum(pca.explained_variance_ratio_))

plt.bar(list(range(1,14)),pca.explained_variance_ratio_,alpha=0.5, align='center')
plt.ylabel('Variation explained')
plt.xlabel('eigen Value')
plt.grid(True)
plt.show()

fig = plt.figure(figsize=(10,6))
plt.step(list(range(1,14)),pca.explained_variance_ratio_.cumsum(),  where="mid",marker="o", linestyle = "--", linewidth=2)
plt.title('Explained variance by components')
plt.xlabel('number of components')
plt.ylabel("cumulative explained variance")
plt.grid(True)
plt.show()

pca=PCA(n_components=5, random_state=1)
pca5_df = pd.DataFrame(pca.fit_transform(df_scaled), columns=["PC1" , "PC2" , "PC3", "PC4", "PC5"])

pca5_df.head()

plt.figure(figsize=(15, 7))
sns.heatmap(
    pca5_df.corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()

clusters = range(1, 10)
meanDistortions = []

for k in clusters:
    model = KMeans(n_clusters=k, random_state=2)
    model.fit(pca5_df)
    prediction = model.predict(pca5_df)
    distortion = (
        sum(
            np.min(cdist(pca5_df, model.cluster_centers_, "euclidean"), axis=1)
        )
        / pca5_df.shape[0]
    )

    meanDistortions.append(distortion)

    #print("Number of Clusters:", k, "\tAverage Distortion:", distortion)

plt.plot(clusters, meanDistortions, "bx-")
plt.xlabel("k")
plt.ylabel("Average distortion")
plt.title("Selecting k with the Elbow Method")
plt.grid(True)
plt.show()

silhouette_coefficients = []
for k in range (2, 11):
    kmeans = KMeans (n_clusters = k, random_state=2)
    kmeans.fit (pca5_df)
    score = silhouette_score (pca5_df, kmeans.labels_)
    silhouette_coefficients.append (score)

# Plotting the results:
plt.plot (range (2, 11), silhouette_coefficients)
plt.xticks (range (2, 11))
plt.xlabel ("Number of Clusters")
plt.ylabel ("Silhouette Coefficient")
plt.grid(True)
plt.show ()

# Create a KMeans object with 6 clusters
kmeans_pca = KMeans(n_clusters=6, random_state=1)

# Fit the KMeans algorithm on the PCA-transformed data
kmeans_pca.fit(pca5_df)

# Assign cluster labels to the PCA scores
pca5_df["PCA_kmeans_clusters"] = kmeans_pca.labels_

pca5_df.head()

pca5_df.PCA_kmeans_clusters.value_counts ()

fig = plt.figure(figsize=(12,5), dpi=100)
sns.scatterplot(x='PC1', y='PC2', data=pca5_df, hue='PCA_kmeans_clusters', palette=['blue', 'green', 'red','yellow','brown','black'])
plt.title('PC1 vs PC2', fontsize=12)
plt.xticks(rotation=90, fontsize=10)
plt.yticks(fontsize=10)
plt.xlabel('PC1', fontsize=10)
plt.ylabel('PC2', fontsize=10)
plt.show()

#Appending cluster lables based on PCA to the original data frame
df_scaled["PCA_kmeans_clusters"]=kmeans_pca.labels_

cluster_profile_new = df_scaled.groupby("PCA_kmeans_clusters").mean().drop(columns=["kmeans_clusters"])

# let's display cluster profiles
cluster_profile_new.style.highlight_max(color="lightblue", axis=0)

df_scaled.groupby("PCA_kmeans_clusters").mean().drop(columns=["kmeans_clusters"]).plot.bar(figsize=(15, 8))

